{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "advisory-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import shap\n",
    "import argparse\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoModelForPreTraining,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AlbertForSequenceClassification,\n",
    "    MT5ForConditionalGeneration,\n",
    "    AutoModelWithLMHead,\n",
    "    AutoTokenizer,\n",
    "    AlbertTokenizer,\n",
    "    T5Tokenizer,\n",
    "    PretrainedConfig,\n",
    "    PreTrainedTokenizer,\n",
    ")\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import LayerIntegratedGradients, GradientShap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cooked-blind",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#kb_bert = 'KB/bert-base-swedish-cased'\n",
    "#kb_bert = 'xlm-roberta-base'\n",
    "kb_bert = 'bert-base-multilingual-cased'\n",
    "\n",
    "#kb_bert = 'KB/electra-base-swedish-cased-discriminator'\n",
    "tokenizer = AutoTokenizer.from_pretrained(kb_bert)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(kb_bert)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model.load_state_dict(torch.load(\"/workspace/models/KB/bert-base-swedish-cased.pt\"))\n",
    "#model.load_state_dict(torch.load(\"/workspace/models/KB/electra-base-swedish-cased-discriminator_ft.pt\"))\n",
    "#model.load_state_dict(torch.load(\"/workspace/models/xlm-roberta-base_ft.pt\"))\n",
    "model.load_state_dict(torch.load(\"/workspace/models/bert-base-multilingual-cased_ft.pt\"))\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "threatened-lighting",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-3eef0e6aaffbff08\n",
      "Reusing dataset csv (/.cache/huggingface/datasets/csv/default-3eef0e6aaffbff08/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fc760de9b64bbdb96f2d908038878d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2041.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_dataset = load_dataset(\"csv\", data_files='/workspace/models/test2.csv')\n",
    "test_ind = test_dataset\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], max_length = 512, add_special_tokens = True)\n",
    "\n",
    "test_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "preliminary-extreme",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModelWrapper(nn.Module):\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        super(BertModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, input_ids):        \n",
    "        outputs = self.model(input_ids=input_ids)\n",
    "        logits = outputs.logits\n",
    "        return nn.functional.softmax(logits, dim=1)\n",
    "\n",
    "def tokens2words(tokens, seq, token_prefix=\"##\"):\n",
    "    \"\"\"\n",
    "    Utility function to aggregate 'seq' on word-level based on 'tokens'\n",
    "    \"\"\"\n",
    "\n",
    "    tmp = []\n",
    "    for token, x in zip(tokens, seq):\n",
    "        if token.startswith(token_prefix):\n",
    "            if type(x) == str:\n",
    "                x = x.replace(token_prefix,\"\")\n",
    "            tmp[-1] += x\n",
    "        else:\n",
    "            if type(x) == str:\n",
    "                tmp.append(x)\n",
    "            else:\n",
    "                tmp.append(x.item())\n",
    "\n",
    "    return tmp if type(tmp[-1]) == str else torch.tensor(tmp, device=device)\n",
    "\n",
    "def add_attributions_to_visualizer(attributions, pred, pred_ind, label, tokens, delta, vis_data_records):\n",
    "    vis_data_records.append(viz.VisualizationDataRecord(\n",
    "                            attributions/np.linalg.norm(attributions),\n",
    "                            pred,\n",
    "                            pred_ind,\n",
    "                            label,\n",
    "                            pred_ind,\n",
    "                            attributions.sum(),       \n",
    "                            tokens,\n",
    "                            delta)) \n",
    "\n",
    "def input_ref(sentence):\n",
    "    input_ids = torch.tensor(sentence, device=device)\n",
    "    \n",
    "    ref_token_id = tokenizer.pad_token_id # A token used for generating token reference\n",
    "    baseline = input_ids.clone()\n",
    "    baseline[:,1:-1] = ref_token_id \n",
    "    return input_ids, baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "lesser-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x = test_dataset['train']\n",
    "input_text = input_x['text']\n",
    "attention_mask = input_x['attention_mask']\n",
    "label = input_x['label']\n",
    "input_ids = input_x['input_ids']\n",
    "\n",
    "#pred = predict_fn(input_ids=input_ids, attention_mask=attention_mask, output_logits=True)\n",
    "#sns.distplot(pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "rental-corruption",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertForSequenceClassification' object has no attribute 'roberta'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-bcc43362b99c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbert_model_wrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertModelWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLayerIntegratedGradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_model_wrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_model_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroberta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# accumalate couple samples in this array for visualization purposes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvis_data_records_ig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    592\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 594\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BertForSequenceClassification' object has no attribute 'roberta'"
     ]
    }
   ],
   "source": [
    "bert_model_wrapper = BertModelWrapper(model)\n",
    "\n",
    "lig = LayerIntegratedGradients(bert_model_wrapper, bert_model_wrapper.model.roberta.embeddings)\n",
    "# accumalate couple samples in this array for visualization purposes\n",
    "vis_data_records_ig = []\n",
    "\n",
    "bert_model_wrapper.eval()\n",
    "bert_model_wrapper.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-divide",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 500\n",
    "torch.cuda.empty_cache()\n",
    "#idx = 344, 673,1895, 1537\n",
    "idx = 1537\n",
    "#idx = np.random.choice(len(test_dataset['train']))\n",
    "\n",
    "input_x = test_dataset['train']\n",
    "input_text = input_x['text'][idx]\n",
    "label = input_x['label'][idx]\n",
    "input_ids = input_x['input_ids'][idx].unsqueeze(0)\n",
    "attention_mask = input_x['attention_mask'][idx].unsqueeze(0)\n",
    "\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "input_words = tokens2words(input_tokens, input_tokens)\n",
    "\n",
    "input_ids, baseline = input_ref(input_ids)\n",
    "#pred = bert_model_wrapper(input_ids)[:, 1].unsqueeze(1).item()\n",
    "pred = bert_model_wrapper(input_ids)\n",
    "pred_label = pred.argmax()\n",
    "pred_p = pred[0, pred_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-ladder",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions, delta = lig.attribute(inputs=input_ids, n_steps=n_steps,\n",
    "                                   baselines=baseline,\n",
    "                                    internal_batch_size=16,\n",
    "                                    return_convergence_delta=True,\n",
    "                                    target=pred_label\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-combining",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing couple samples in an array for visualization purposes\n",
    "att = attributions.sum(dim=2).squeeze(0)\n",
    "#attributions = attributions / torch.norm(attributions)\n",
    "att = att.detach().cpu().clone().numpy()\n",
    "\n",
    "phi_words = tokens2words(input_tokens, att)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-therapy",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('pred: ', pred_label.item(), '(', '%.2f' % pred_p.item(), ')', ', delta: ', abs(delta.item()))\n",
    "\n",
    "add_attributions_to_visualizer(att, pred_p, pred_label, label, input_words, delta, vis_data_records_ig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-lambda",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.visualize_text(vis_data_records_ig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-victorian",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x = test_dataset[\"train\"]\n",
    "n_steps = 500\n",
    "torch.cuda.empty_cache()\n",
    "ig_val = []\n",
    "\n",
    "for i in tqdm(range(len(input_x))):\n",
    "    input_ids = input_x['input_ids'][i].unsqueeze(0)\n",
    "    input_ids, baseline = input_ref(input_ids)\n",
    "\n",
    "    attributions, delta = lig.attribute(inputs=input_ids, n_steps=n_steps,\n",
    "                                   baselines=baseline,\n",
    "                                    internal_batch_size=8,\n",
    "                                    return_convergence_delta=True,\n",
    "                                    target=1\n",
    "                                   )\n",
    "    ig_val.append(attributions)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-portfolio",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Store data (serialize)\n",
    "with open('/workspace/models/'+ kb_bert +'_ig.pickle', 'wb') as handle:\n",
    "    pickle.dump(ig_val, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Load data (deserialize)\n",
    "with open('/workspace/models/'+ kb_bert +'_ig.pickle', 'rb') as handle:\n",
    "    unserialized_data = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_val[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-reference",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = []\n",
    "features = {}\n",
    "for i in range(len(input_x)):\n",
    "    input_ids = input_x['input_ids'][i]\n",
    "    input_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    input_words = tokens2words(input_tokens, input_tokens)\n",
    "    ig_val_sum = ig_val[i].sum(dim=2).squeeze(0)\n",
    "    ig_val_sum = ig_val_sum.detach().cpu().clone().numpy()\n",
    "    phi_words = tokens2words(input_tokens, ig_val_sum)\n",
    "    for j in range(len(input_words)):\n",
    "        if input_words[j] in features.keys():\n",
    "            old_val = features[input_words[j]]\n",
    "            features[input_words[j]] = ((phi_words[j]).item() + old_val[0], old_val[1]+1)\n",
    "        else:\n",
    "            features[input_words[j]] = ((phi_words[j]).item(), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-license",
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: v for k, v in sorted(features.items(), key=lambda item: item[1][0], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-tutorial",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
